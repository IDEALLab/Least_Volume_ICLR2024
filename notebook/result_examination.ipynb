{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys; sys.path.append('../src/'); sys.path.append('..')\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.utils.metrics import l2_loss, explained_reconstruction, mean_correlation, importance_correlation, main_exprec\n",
    "from src.least_volume_image import *\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.utils.data import DataLoader\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_dic = {\n",
    "    'vol': VolumeAE_BCE,\n",
    "    'l1': L1AE_BCE,\n",
    "    'lasso': LassoAE_BCE,\n",
    "    'bce': BCEAutoencoder\n",
    "}\n",
    "\n",
    "def load_model(ae_name, json_dir, tar_dir, lam, lip=True, device='cpu'):\n",
    "    with open(json_dir) as f: configs = json.load(f)\n",
    "    AE = ae_dic[ae_name]\n",
    "    Decoder = TrueSNDCGeneratorSig if lip else DCGeneratorSig\n",
    "    model = AE(configs, DCDiscriminator, Decoder, Adam, weights=[1., lam]).to(device)\n",
    "    model.load(tar_dir)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_dataset(name, train=True, device='cpu'):\n",
    "    dataset, _ = load_dataset(name, train=train, device=device)\n",
    "    dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "    return next(iter(dataloader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_metrics(model, dataset, metrics):\n",
    "    return [metric(model, dataset) for metric in metrics]\n",
    "\n",
    "def ae_statistics(data_name, ae_name, group, epoch, lams, metrics, eps=None, lip=True, comment='', train=True, device='cpu', src='../saves/image/'):\n",
    "    if not lip: comment = comment + '_nolip'\n",
    "    if ae_name == 'vol' and eps is not None: comment = '_e{}'.format(eps) + comment\n",
    "\n",
    "    dataset = get_dataset(data_name, train=train, device=device)\n",
    "    stats = []\n",
    "    for lam in lams:\n",
    "        dir = os.path.join(src, data_name, group, '{}_{}{}/'.format(ae_name, lam, comment))\n",
    "        print(dir)\n",
    "        json_file = glob.glob('*.json', root_dir=dir)[0]\n",
    "        json_dir = os.path.join(dir, json_file)\n",
    "        tar_file = glob.glob('*{}.tar'.format(epoch), root_dir=dir)[0]\n",
    "        tar_dir = os.path.join(dir, tar_file)\n",
    "\n",
    "        model = load_model(ae_name, json_dir, tar_dir, lam, lip, device)\n",
    "        stats.append(get_metrics(model, dataset, metrics))\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "\n",
    "def prune(k, z, descending):\n",
    "    std, idx = z.std(0).sort(descending=descending)\n",
    "    mean = z.mean(0)\n",
    "    z[:, idx[:k]] = mean[idx[:k]]\n",
    "    return z\n",
    "\n",
    "def l2_prune(k=0, descending=True):\n",
    "    def _l2_(model, dataset):\n",
    "        z = model.encode(dataset)\n",
    "        z = prune(k, z, descending)\n",
    "        rec = model.decode(z)\n",
    "        return l2_loss(dataset, rec)\n",
    "    return _l2_\n",
    "\n",
    "def l2_ps(model, dataset):\n",
    "    z = model.encode(dataset)\n",
    "    std, idx = z.std(0).sort(descending=True)\n",
    "    mean = z.mean(0)\n",
    "    l2s = []\n",
    "    for i in tqdm(idx):\n",
    "        z_ = z.clone()\n",
    "        z_[:, i] = mean[i]\n",
    "        rec = model.decode(z_)\n",
    "        l2s.append(l2_loss(dataset, rec))\n",
    "    return torch.stack(l2s)\n",
    "\n",
    "def l2_cum(descending=True):\n",
    "    def _l2_(model, dataset):\n",
    "        l2s = []\n",
    "        z = model.encode(dataset)\n",
    "        for i in trange(z.size(1)):\n",
    "            _l2 = l2_prune(k=i+1, descending=descending)\n",
    "            l2s.append(_l2(model, dataset))\n",
    "        return torch.stack(l2s)\n",
    "    return _l2_\n",
    "\n",
    "def z_std(model, dataset):\n",
    "    z = model.encode(dataset)\n",
    "    std, idx = z.std(0).sort(descending=True)\n",
    "    return std\n",
    "\n",
    "def z_index(model, dataset):\n",
    "    z = model.encode(dataset)\n",
    "    std, idx = z.std(0).sort(descending=True)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [l2_prune(0), l2_prune(None), l2_ps, l2_cum(True), l2_cum(False), z_index]\n",
    "names = ['l2_non', 'l2_all', 'l2_each', 'l2_cum_d', 'l2_cum_a', 'z_index']\n",
    "\n",
    "dataset_name = 'syn'\n",
    "group = 'lasso'\n",
    "ae_name = 'lasso'\n",
    "\n",
    "stats = ae_statistics(dataset_name, ae_name, group=group, epoch=399, lams=(3e-2, 1e-2, 3e-3, 1e-3, 3e-4), metrics=metrics, device='cuda:1')\n",
    "\n",
    "path = os.path.join('../saves/image/', dataset_name, group)\n",
    "for i, nm in enumerate(names):\n",
    "    ls = []\n",
    "    for each in stats:\n",
    "        ls.append(each[i])\n",
    "    np.save(os.path.join(path, '{}.npy'.format(nm)), torch.stack(ls).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../saves/image/syn/lasso/lasso_0.01/\n",
      "../saves/image/syn/lasso/lasso_0.003/\n",
      "../saves/image/syn/lasso/lasso_0.001/\n",
      "../saves/image/syn/lasso/lasso_0.0003/\n",
      "../saves/image/syn/lasso/lasso_0.0001/\n",
      "../saves/image/syn/l1/l1_0.01/\n",
      "../saves/image/syn/l1/l1_0.003/\n",
      "../saves/image/syn/l1/l1_0.001/\n",
      "../saves/image/syn/l1/l1_0.0003/\n",
      "../saves/image/syn/l1/l1_0.0001/\n",
      "../saves/image/mnist/lasso/lasso_0.03/\n",
      "../saves/image/mnist/lasso/lasso_0.01/\n",
      "../saves/image/mnist/lasso/lasso_0.003/\n",
      "../saves/image/mnist/lasso/lasso_0.001/\n",
      "../saves/image/mnist/lasso/lasso_0.0003/\n",
      "../saves/image/mnist/l1/l1_0.03/\n",
      "../saves/image/mnist/l1/l1_0.01/\n",
      "../saves/image/mnist/l1/l1_0.003/\n",
      "../saves/image/mnist/l1/l1_0.001/\n",
      "../saves/image/mnist/l1/l1_0.0003/\n",
      "Files already downloaded and verified\n",
      "../saves/image/cifar10/lasso/lasso_0.03/\n",
      "../saves/image/cifar10/lasso/lasso_0.01/\n",
      "../saves/image/cifar10/lasso/lasso_0.003/\n",
      "../saves/image/cifar10/lasso/lasso_0.001/\n",
      "../saves/image/cifar10/lasso/lasso_0.0003/\n",
      "Files already downloaded and verified\n",
      "../saves/image/cifar10/l1/l1_0.03/\n",
      "../saves/image/cifar10/l1/l1_0.01/\n",
      "../saves/image/cifar10/l1/l1_0.003/\n",
      "../saves/image/cifar10/l1/l1_0.001/\n",
      "../saves/image/cifar10/l1/l1_0.0003/\n"
     ]
    }
   ],
   "source": [
    "metrics = [z_std] #[l2_prune(0), l2_prune(None), l2_ps, l2_cum(True), l2_cum(False), z_index]\n",
    "names = ['z_std'] #['l2_non', 'l2_all', 'l2_each', 'l2_cum_d', 'l2_cum_a', 'z_index']\n",
    "\n",
    "for dataset_name, lams, epoch in zip(['syn', 'mnist', 'cifar10'], \\\n",
    "                                     [(1e-2, 3e-3, 1e-3, 3e-4, 1e-4), (3e-2, 1e-2, 3e-3, 1e-3, 3e-4), (3e-2, 1e-2, 3e-3, 1e-3, 3e-4)], [399, 399, 999]):\n",
    "    for group in ['lasso', 'l1']:\n",
    "        ae_name = group\n",
    "\n",
    "        stats = ae_statistics(dataset_name, ae_name, group=group, epoch=epoch, lams=lams, metrics=metrics, device='cuda:7')\n",
    "\n",
    "        path = os.path.join('../saves/image/', dataset_name, group)\n",
    "        for i, nm in enumerate(names):\n",
    "            ls = []\n",
    "            for each in stats:\n",
    "                ls.append(each[i])\n",
    "            np.save(os.path.join(path, '{}.npy'.format(nm)), torch.stack(ls).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../saves/image/syn/non/bce_0.0/\n",
      "../saves/image/syn/non/bce_0.0_nolip/\n",
      "../saves/image/mnist/non/bce_0.0/\n",
      "../saves/image/mnist/non/bce_0.0_nolip/\n",
      "Files already downloaded and verified\n",
      "../saves/image/cifar10/non/bce_0.0/\n",
      "Files already downloaded and verified\n",
      "../saves/image/cifar10/non/bce_0.0_nolip/\n"
     ]
    }
   ],
   "source": [
    "metrics = [z_std] # [l2_prune(0), l2_prune(None), l2_ps, l2_cum(True), l2_cum(False), z_index]\n",
    "names = ['z_std'] # ['l2_non', 'l2_all', 'l2_each', 'l2_cum_d', 'l2_cum_a', 'z_index']\n",
    "group = 'non'\n",
    "ae_name = 'bce'\n",
    "\n",
    "for dataset_name, lams, epoch in zip(['syn', 'mnist', 'cifar10'], \\\n",
    "        [(0., ), (0., ), (0., )], [399, 399, 999]):\n",
    "    for lip in [True, False]:\n",
    "        stats = ae_statistics(dataset_name, ae_name, group=group, epoch=epoch, lams=lams, lip=lip, metrics=metrics, device='cuda:7')\n",
    "\n",
    "        path = os.path.join('../saves/image/', dataset_name, group)\n",
    "        for i, nm in enumerate(names):\n",
    "            ls = []\n",
    "            for each in stats:\n",
    "                ls.append(each[i])\n",
    "            np.save(os.path.join(path, '{}_{}.npy'.format(lip, nm)), torch.stack(ls).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../saves/image/syn/nolip/lasso_0.001_nolip/\n",
      "../saves/image/syn/nolip/l1_0.001_nolip/\n",
      "../saves/image/syn/nolip/vol_0.001_e1.0_nolip/\n",
      "../saves/image/mnist/nolip/lasso_0.003_nolip/\n",
      "../saves/image/mnist/nolip/l1_0.003_nolip/\n",
      "../saves/image/mnist/nolip/vol_0.003_e1.0_nolip/\n",
      "Files already downloaded and verified\n",
      "../saves/image/cifar10/nolip/lasso_0.003_nolip/\n",
      "Files already downloaded and verified\n",
      "../saves/image/cifar10/nolip/l1_0.003_nolip/\n",
      "Files already downloaded and verified\n",
      "../saves/image/cifar10/nolip/vol_0.003_e1.0_nolip/\n"
     ]
    }
   ],
   "source": [
    "metrics = [z_std] # [l2_prune(0), l2_prune(None), l2_ps, l2_cum(True), l2_cum(False), z_index]\n",
    "names = ['z_std'] #['l2_non', 'l2_all', 'l2_each', 'l2_cum_d', 'l2_cum_a', 'z_index']\n",
    "group = 'nolip'\n",
    "\n",
    "for dataset_name, lams, epoch in zip(['syn', 'mnist', 'cifar10'], \\\n",
    "        [(1e-3, ), (3e-3, ), (3e-3, )], [399, 399, 999]):\n",
    "    for ae_name in ['lasso', 'l1', 'vol']:\n",
    "        stats = ae_statistics(dataset_name, ae_name, group=group, epoch=epoch, eps=1., lams=lams, lip=False, metrics=metrics, device='cuda:7')\n",
    "\n",
    "        path = os.path.join('../saves/image/', dataset_name, group)\n",
    "        for i, nm in enumerate(names):\n",
    "            ls = []\n",
    "            for each in stats:\n",
    "                ls.append(each[i])\n",
    "            np.save(os.path.join(path, '{}_{}.npy'.format(ae_name, nm)), torch.stack(ls).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "metrics = [z_std] #[l2_prune(0), l2_prune(None), l2_ps, l2_cum(True), l2_cum(False), z_index]\n",
    "names = ['z_std'] # ['l2_non', 'l2_all', 'l2_each', 'l2_cum_d', 'l2_cum_a', 'z_index']\n",
    "\n",
    "group = 'vol'\n",
    "ae_name = 'vol'\n",
    "\n",
    "def vol_main(device, eps):\n",
    "    for dataset_name, lams, epoch in zip(['syn', 'mnist', 'cifar10'], \\\n",
    "                                        [(1e-2, 3e-3, 1e-3, 3e-4, 1e-4), \\\n",
    "                                        (3e-2, 1e-2, 3e-3, 1e-3, 3e-4), \\\n",
    "                                        (3e-2, 1e-2, 3e-3, 1e-3, 3e-4)], \\\n",
    "                                        [399, 399, 999]):\n",
    "        stats = ae_statistics(dataset_name, ae_name, group=group, epoch=epoch, lams=lams, eps=eps, metrics=metrics, device='cuda:{}'.format(device))\n",
    "\n",
    "        path = os.path.join('../saves/image/', dataset_name, group)\n",
    "        for i, nm in enumerate(names):\n",
    "            ls = []\n",
    "            for each in stats:\n",
    "                ls.append(each[i])\n",
    "            np.save(os.path.join(path, 'e{}_{}.npy'.format(eps, nm)), torch.stack(ls).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, eps in zip(range(5), [0., 1., 3., 10., 30.]):\n",
    "    p = mp.Process(target=vol_main, args=(i, eps))\n",
    "    p.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toilet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
