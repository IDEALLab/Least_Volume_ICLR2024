{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys; sys.path.append('../src/'); sys.path.append('..')\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.utils.metrics import l2_loss, explained_reconstruction, mean_correlation, importance_correlation, main_exprec\n",
    "from src.train_least_volume import *\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.utils.data import DataLoader\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_dic = {\n",
    "    'vol': VolumeAE_BCE,\n",
    "    'l1': L1AE_BCE,\n",
    "    'lasso': LassoAE_BCE,\n",
    "    'bce': BCEAutoencoder\n",
    "}\n",
    "\n",
    "def load_model(ae_name, json_dir, tar_dir, lam, lip=True, device='cpu'):\n",
    "    with open(json_dir) as f: configs = json.load(f)\n",
    "    AE = ae_dic[ae_name]\n",
    "    Decoder = TrueSNDCGeneratorSig if lip else DCGeneratorSig\n",
    "    model = AE(configs, DCDiscriminator, Decoder, Adam, weights=[1., lam]).to(device)\n",
    "    model.load(tar_dir)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_dataset(name, train=True, device='cpu', batch_size=None):\n",
    "    dataset, _ = load_dataset(name, train=train, device=device)\n",
    "    batch_size = len(dataset) if batch_size is None else batch_size\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return dataloader\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_metrics(model, dataloader, metrics):\n",
    "    return [metric(model, dataloader) for metric in metrics]\n",
    "\n",
    "def ae_statistics(data_name, ae_name, group, epoch, lams, metrics, eps=None, lip=True, comment='', cv='cv1', train=True, device='cpu', src='../saves/image/'):\n",
    "    if not lip: comment = comment + '_nolip'\n",
    "    if ae_name == 'vol' and eps is not None: comment = '_e{}'.format(eps) + comment\n",
    "\n",
    "    dataloader = get_dataset(data_name, train=train, device=device)\n",
    "    stats = []\n",
    "    for lam in lams:\n",
    "        dir = os.path.join(src, data_name, cv, group, '{}_{}{}/'.format(ae_name, lam, comment))\n",
    "        print(dir)\n",
    "        json_file = glob.glob('*.json', root_dir=dir)[0]\n",
    "        json_dir = os.path.join(dir, json_file)\n",
    "        tar_file = glob.glob('*{}.tar'.format(epoch), root_dir=dir)[0]\n",
    "        tar_dir = os.path.join(dir, tar_file)\n",
    "\n",
    "        model = load_model(ae_name, json_dir, tar_dir, lam, lip, device)\n",
    "        stats.append(get_metrics(model, dataloader, metrics))\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "\n",
    "def prune(k, z, descending):\n",
    "    std, idx = z.std(0).sort(descending=descending)\n",
    "    mean = z.mean(0)\n",
    "    z[:, idx[:k]] = mean[idx[:k]]\n",
    "    return z\n",
    "\n",
    "def l2_prune(k=0, descending=True):\n",
    "    def _l2_(model, dataloader):\n",
    "        z = torch.cat([model.encode(batch) for batch in dataloader])\n",
    "        z = prune(k, z, descending)\n",
    "        rec = model.decode(z)\n",
    "        return l2_loss(dataset, rec)\n",
    "    return _l2_\n",
    "\n",
    "def l2_ps(model, dataset):\n",
    "    z = model.encode(dataset)\n",
    "    std, idx = z.std(0).sort(descending=True)\n",
    "    mean = z.mean(0)\n",
    "    l2s = []\n",
    "    for i in tqdm(idx):\n",
    "        z_ = z.clone()\n",
    "        z_[:, i] = mean[i]\n",
    "        rec = model.decode(z_)\n",
    "        l2s.append(l2_loss(dataset, rec))\n",
    "    return torch.stack(l2s)\n",
    "\n",
    "def l2_cum(descending=True):\n",
    "    def _l2_(model, dataset):\n",
    "        l2s = []\n",
    "        z = model.encode(dataset)\n",
    "        for i in trange(z.size(1)):\n",
    "            _l2 = l2_prune(k=i+1, descending=descending)\n",
    "            l2s.append(_l2(model, dataset))\n",
    "        return torch.stack(l2s)\n",
    "    return _l2_\n",
    "\n",
    "def z_std(model, dataloader):\n",
    "    z = torch.cat([model.encode(batch) for batch in dataloader])\n",
    "    std, idx = z.std(0).sort(descending=True)\n",
    "    return std\n",
    "\n",
    "def z_index(model, dataloader):\n",
    "    z = torch.cat([model.encode(batch) for batch in dataloader])\n",
    "    std, idx = z.std(0).sort(descending=True)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [l2_prune(0), l2_prune(None), l2_ps, l2_cum(True), l2_cum(False), z_index, z_std]\n",
    "names = ['l2_non', 'l2_all', 'l2_each', 'l2_cum_a', 'z_index', 'z_std']\n",
    "#['l2_non', 'l2_all', 'l2_each', 'l2_cum_d', 'l2_cum_a', 'z_index']\n",
    "\n",
    "dataset_name = 'syn'\n",
    "group = 'lasso'\n",
    "ae_name = 'lasso'\n",
    "\n",
    "stats = ae_statistics(dataset_name, ae_name, group=group, epoch=399, lams=(1e-2, 3e-3, 1e-3, 3e-4, 1e-4), metrics=metrics, device='cuda:7') # (3e-2, 1e-2, 3e-3, 1e-3, 3e-4)\n",
    "\n",
    "path = os.path.join('../saves/image/', dataset_name, 'cv1', group)\n",
    "for i, nm in enumerate(names):\n",
    "    ls = []\n",
    "    for each in stats:\n",
    "        ls.append(each[i])\n",
    "    np.save(os.path.join(path, '{}.npy'.format(nm)), torch.stack(ls).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../saves/image/celeba/cv0/lasso/lasso_0.03/\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 79.48 GiB (GPU 6; 79.15 GiB total capacity; 7.89 GiB already allocated; 70.86 GiB free; 7.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mlasso\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39ml1\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     12\u001b[0m     ae_name \u001b[39m=\u001b[39m group\n\u001b[0;32m---> 14\u001b[0m     stats \u001b[39m=\u001b[39m ae_statistics(dataset_name, ae_name, group\u001b[39m=\u001b[39;49mgroup, epoch\u001b[39m=\u001b[39;49mepoch, cv\u001b[39m=\u001b[39;49mcv, lams\u001b[39m=\u001b[39;49mlams, metrics\u001b[39m=\u001b[39;49mmetrics, device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda:6\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     16\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m../saves/image/\u001b[39m\u001b[39m'\u001b[39m, dataset_name, cv, group)\n\u001b[1;32m     17\u001b[0m     \u001b[39mfor\u001b[39;00m i, nm \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(names):\n",
      "Cell \u001b[0;32mIn[14], line 41\u001b[0m, in \u001b[0;36mae_statistics\u001b[0;34m(data_name, ae_name, group, epoch, lams, metrics, eps, lip, comment, cv, train, device, src)\u001b[0m\n\u001b[1;32m     38\u001b[0m     tar_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mdir\u001b[39m, tar_file)\n\u001b[1;32m     40\u001b[0m     model \u001b[39m=\u001b[39m load_model(ae_name, json_dir, tar_dir, lam, lip, device)\n\u001b[0;32m---> 41\u001b[0m     stats\u001b[39m.\u001b[39mappend(get_metrics(model, dataset, metrics))\n\u001b[1;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m stats\n",
      "File \u001b[0;32m~/.conda/envs/toilet/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m, in \u001b[0;36mget_metrics\u001b[0;34m(model, dataset, metrics)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_metrics\u001b[39m(model, dataset, metrics):\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m [metric(model, dataset) \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m metrics]\n",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_metrics\u001b[39m(model, dataset, metrics):\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m [metric(model, dataset) \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m metrics]\n",
      "Cell \u001b[0;32mIn[30], line 11\u001b[0m, in \u001b[0;36ml2_prune.<locals>._l2_\u001b[0;34m(model, dataset)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_l2_\u001b[39m(model, dataset):\n\u001b[0;32m---> 11\u001b[0m     z \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(dataset)\n\u001b[1;32m     12\u001b[0m     z \u001b[39m=\u001b[39m prune(k, z, descending)\n\u001b[1;32m     13\u001b[0m     rec \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecode(z)\n",
      "File \u001b[0;32m~/GitHub/icml_least_volume/notebook/../src/model/autoencoder.py:29\u001b[0m, in \u001b[0;36m_AutoEncoder.encode\u001b[0;34m(self, x_batch)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, x_batch):\n\u001b[0;32m---> 29\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x_batch)\n",
      "File \u001b[0;32m~/.conda/envs/toilet/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/GitHub/icml_least_volume/notebook/../src/model/cmpnts.py:218\u001b[0m, in \u001b[0;36mDCDiscriminator.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    219\u001b[0m     critics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritics(x)\n\u001b[1;32m    220\u001b[0m     \u001b[39mreturn\u001b[39;00m critics\n",
      "File \u001b[0;32m~/GitHub/icml_least_volume/notebook/../src/model/cmpnts.py:187\u001b[0m, in \u001b[0;36mConv2DNetwork.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m--> 187\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/.conda/envs/toilet/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/toilet/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/toilet/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/GitHub/icml_least_volume/notebook/../src/model/layers.py:120\u001b[0m, in \u001b[0;36m_Combo.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/.conda/envs/toilet/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/toilet/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/toilet/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/toilet/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.conda/envs/toilet/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 79.48 GiB (GPU 6; 79.15 GiB total capacity; 7.89 GiB already allocated; 70.86 GiB free; 7.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "metrics = [l2_prune(0), l2_prune(None), l2_ps, l2_cum(False), z_index, z_std]\n",
    "names = ['l2_non', 'l2_all', 'l2_each', 'l2_cum_a', 'z_index', 'z_std']\n",
    "cv = 'cv0'\n",
    "\n",
    "for dataset_name, lams, epoch in list(zip(['syn', 'mnist', 'cifar10', 'celeba'], \\\n",
    "                                     [(1e-2, 3e-3, 1e-3, 3e-4, 1e-4), \n",
    "                                      (3e-2, 1e-2, 3e-3, 1e-3, 3e-4), \n",
    "                                      (3e-2, 1e-2, 3e-3, 1e-3, 3e-4),\n",
    "                                      (3e-2, 1e-2, 3e-3, 1e-3, 3e-4)], \n",
    "                                      [399, 399, 999, 299]))[3:]:\n",
    "    for group in ['lasso', 'l1']:\n",
    "        ae_name = group\n",
    "\n",
    "        stats = ae_statistics(dataset_name, ae_name, group=group, epoch=epoch, cv=cv, lams=lams, metrics=metrics, device='cuda:6')\n",
    "\n",
    "        path = os.path.join('../saves/image/', dataset_name, cv, group)\n",
    "        for i, nm in enumerate(names):\n",
    "            ls = []\n",
    "            for each in stats:\n",
    "                ls.append(each[i])\n",
    "            np.save(os.path.join(path, '{}.npy'.format(nm)), torch.stack(ls).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../saves/image/syn/nolip/lasso_0.001_nolip/\n",
      "../saves/image/syn/nolip/l1_0.001_nolip/\n",
      "../saves/image/syn/nolip/vol_0.001_e1.0_nolip/\n",
      "../saves/image/mnist/nolip/lasso_0.003_nolip/\n",
      "../saves/image/mnist/nolip/l1_0.003_nolip/\n",
      "../saves/image/mnist/nolip/vol_0.003_e1.0_nolip/\n",
      "Files already downloaded and verified\n",
      "../saves/image/cifar10/nolip/lasso_0.003_nolip/\n",
      "Files already downloaded and verified\n",
      "../saves/image/cifar10/nolip/l1_0.003_nolip/\n",
      "Files already downloaded and verified\n",
      "../saves/image/cifar10/nolip/vol_0.003_e1.0_nolip/\n"
     ]
    }
   ],
   "source": [
    "metrics = [z_std] # [l2_prune(0), l2_prune(None), l2_ps, l2_cum(True), l2_cum(False), z_index]\n",
    "names = ['z_std'] #['l2_non', 'l2_all', 'l2_each', 'l2_cum_d', 'l2_cum_a', 'z_index']\n",
    "group = 'nolip'\n",
    "\n",
    "for dataset_name, lams, epoch in zip(['syn', 'mnist', 'cifar10'], \\\n",
    "        [(1e-3, ), (3e-3, ), (3e-3, )], [399, 399, 999]):\n",
    "    for ae_name in ['lasso', 'l1', 'vol']:\n",
    "        stats = ae_statistics(dataset_name, ae_name, group=group, epoch=epoch, eps=1., lams=lams, lip=False, metrics=metrics, device='cuda:7')\n",
    "\n",
    "        path = os.path.join('../saves/image/', dataset_name, group)\n",
    "        for i, nm in enumerate(names):\n",
    "            ls = []\n",
    "            for each in stats:\n",
    "                ls.append(each[i])\n",
    "            np.save(os.path.join(path, '{}_{}.npy'.format(ae_name, nm)), torch.stack(ls).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../saves/image/syn/non/bce_0.0/\n",
      "../saves/image/syn/non/bce_0.0_nolip/\n",
      "../saves/image/mnist/non/bce_0.0/\n",
      "../saves/image/mnist/non/bce_0.0_nolip/\n",
      "Files already downloaded and verified\n",
      "../saves/image/cifar10/non/bce_0.0/\n",
      "Files already downloaded and verified\n",
      "../saves/image/cifar10/non/bce_0.0_nolip/\n"
     ]
    }
   ],
   "source": [
    "metrics = [z_std] # [l2_prune(0), l2_prune(None), l2_ps, l2_cum(True), l2_cum(False), z_index]\n",
    "names = ['z_std'] # ['l2_non', 'l2_all', 'l2_each', 'l2_cum_d', 'l2_cum_a', 'z_index']\n",
    "group = 'non'\n",
    "ae_name = 'bce'\n",
    "\n",
    "for dataset_name, lams, epoch in zip(['syn', 'mnist', 'cifar10'], \\\n",
    "        [(0., ), (0., ), (0., )], [399, 399, 999]):\n",
    "    for lip in [True, False]:\n",
    "        stats = ae_statistics(dataset_name, ae_name, group=group, epoch=epoch, lams=lams, lip=lip, metrics=metrics, device='cuda:7')\n",
    "\n",
    "        path = os.path.join('../saves/image/', dataset_name, group)\n",
    "        for i, nm in enumerate(names):\n",
    "            ls = []\n",
    "            for each in stats:\n",
    "                ls.append(each[i])\n",
    "            np.save(os.path.join(path, '{}_{}.npy'.format(lip, nm)), torch.stack(ls).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "metrics = [z_std] #[l2_prune(0), l2_prune(None), l2_ps, l2_cum(True), l2_cum(False), z_index]\n",
    "names = ['z_std'] # ['l2_non', 'l2_all', 'l2_each', 'l2_cum_d', 'l2_cum_a', 'z_index']\n",
    "\n",
    "group = 'vol'\n",
    "ae_name = 'vol'\n",
    "cv = 'cv1'\n",
    "\n",
    "def vol_main(device, eps):\n",
    "    for dataset_name, lams, epoch in list(zip(['syn', 'mnist', 'cifar10'], \\\n",
    "                                        [(1e-2, 3e-3, 1e-3, 3e-4, 1e-4), \\\n",
    "                                        (3e-2, 1e-2, 3e-3, 1e-3, 3e-4), \\\n",
    "                                        (3e-2, 1e-2, 3e-3, 1e-3, 3e-4)], \\\n",
    "                                        [399, 399, 999]))[2:]:\n",
    "        stats = ae_statistics(dataset_name, ae_name, group=group, epoch=epoch, lams=lams, cv=cv, eps=eps, metrics=metrics, device='cuda:{}'.format(device))\n",
    "\n",
    "        path = os.path.join('../saves/image/', dataset_name, cv, group)\n",
    "        for i, nm in enumerate(names):\n",
    "            ls = []\n",
    "            for each in stats:\n",
    "                ls.append(each[i])\n",
    "            np.save(os.path.join(path, 'e{}_{}.npy'.format(eps, nm)), torch.stack(ls).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../saves/image/cifar10/cv1/vol/vol_0.03_e1.0/\n",
      "../saves/image/cifar10/cv1/vol/vol_0.01_e1.0/\n",
      "../saves/image/cifar10/cv1/vol/vol_0.003_e1.0/\n",
      "../saves/image/cifar10/cv1/vol/vol_0.001_e1.0/\n",
      "../saves/image/cifar10/cv1/vol/vol_0.0003_e1.0/\n"
     ]
    }
   ],
   "source": [
    "vol_main(0, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, eps in zip(range(5), [0., 1., 3., 10., 30.]):\n",
    "    p = mp.Process(target=vol_main, args=(i, eps))\n",
    "    p.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toilet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
