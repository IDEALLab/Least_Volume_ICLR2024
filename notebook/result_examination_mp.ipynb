{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys; sys.path.append('../src/'); sys.path.append('..')\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.utils.metrics import l2_loss, explained_reconstruction, mean_correlation, importance_correlation, main_exprec\n",
    "from src.least_volume_image import *\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.utils.data import DataLoader\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_dic = {\n",
    "    'vol': VolumeAE_BCE,\n",
    "    'l1': L1AE_BCE,\n",
    "    'lasso': LassoAE_BCE,\n",
    "    'bce': BCEAutoencoder\n",
    "}\n",
    "\n",
    "def load_model(ae_name, json_dir, tar_dir, lam, lip=True, device='cpu'):\n",
    "    with open(json_dir) as f: configs = json.load(f)\n",
    "    AE = ae_dic[ae_name]\n",
    "    Decoder = TrueSNDCGeneratorSig if lip else DCGeneratorSig\n",
    "    model = AE(configs, DCDiscriminator, Decoder, Adam, weights=[1., lam]).to(device)\n",
    "    model.load(tar_dir)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_dataset(name, train=True, device='cpu'):\n",
    "    dataset, _ = load_dataset(name, train=train, device=device)\n",
    "    dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "    return next(iter(dataloader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_metrics(model, dataset, metrics):\n",
    "    return [metric(model, dataset) for metric in metrics]\n",
    "\n",
    "def ae_statistics(data_name, ae_name, group, epoch, lams, metrics, eps=None, lip=True, comment='', train=True, device='cpu', src='../saves/image/'):\n",
    "    if not lip: comment = comment + '_nolip'\n",
    "    if eps is not None: comment = '_e{}'.format(eps) + comment\n",
    "\n",
    "    dataset = get_dataset(data_name, train=train, device=device)\n",
    "    stats = []\n",
    "    for lam in lams:\n",
    "        dir = os.path.join(src, data_name, group, '{}_{}{}/'.format(ae_name, lam, comment))\n",
    "        print(dir)\n",
    "        json_file = glob.glob('*.json', root_dir=dir)[0]\n",
    "        json_dir = os.path.join(dir, json_file)\n",
    "        tar_file = glob.glob('*{}.tar'.format(epoch), root_dir=dir)[0]\n",
    "        tar_dir = os.path.join(dir, tar_file)\n",
    "\n",
    "        model = load_model(ae_name, json_dir, tar_dir, lam, lip, device)\n",
    "        stats.append(get_metrics(model, dataset, metrics))\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "\n",
    "def prune(k, z, descending):\n",
    "    std, idx = z.std(0).sort(descending=descending)\n",
    "    mean = z.mean(0)\n",
    "    z[:, idx[:k]] = mean[idx[:k]]\n",
    "    return z\n",
    "\n",
    "def l2_prune(k=0, descending=True):\n",
    "    def _l2_(model, dataset):\n",
    "        z = model.encode(dataset)\n",
    "        z = prune(k, z, descending)\n",
    "        rec = model.decode(z)\n",
    "        return l2_loss(dataset, rec)\n",
    "    return _l2_\n",
    "\n",
    "def l2_ps(model, dataset):\n",
    "    z = model.encode(dataset)\n",
    "    std, idx = z.std(0).sort(descending=True)\n",
    "    mean = z.mean(0)\n",
    "    l2s = []\n",
    "    for i in tqdm(idx):\n",
    "        z_ = z.clone()\n",
    "        z_[:, i] = mean[i]\n",
    "        rec = model.decode(z_)\n",
    "        l2s.append(l2_loss(dataset, rec))\n",
    "    return torch.stack(l2s)\n",
    "\n",
    "def l2_cum(descending=True):\n",
    "    def _l2_(model, dataset):\n",
    "        l2s = []\n",
    "        z = model.encode(dataset)\n",
    "        for i in trange(z.size(1)):\n",
    "            _l2 = l2_prune(k=i+1, descending=descending)\n",
    "            l2s.append(_l2(model, dataset))\n",
    "        return torch.stack(l2s)\n",
    "    return _l2_\n",
    "\n",
    "def z_index(model, dataset):\n",
    "    z = model.encode(dataset)\n",
    "    std, idx = z.std(0).sort(descending=True)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "metrics = [l2_prune(0), l2_prune(None), l2_ps, l2_cum(True), l2_cum(False), z_index]\n",
    "names = ['l2_non', 'l2_all', 'l2_each', 'l2_cum_d', 'l2_cum_a', 'z_index']\n",
    "\n",
    "group = 'vol'\n",
    "ae_name = 'vol'\n",
    "\n",
    "def vol_main(device, eps):\n",
    "    for dataset_name, lams, epoch in zip(['syn', 'mnist', 'cifar10'], \\\n",
    "                                        [(1e-2, 3e-3, 1e-3, 3e-4, 1e-4), \\\n",
    "                                        (3e-2, 1e-2, 3e-3, 1e-3, 3e-4), \\\n",
    "                                        (3e-2, 1e-2, 3e-3, 1e-3, 3e-4)], \\\n",
    "                                        [399, 399, 999]):\n",
    "        stats = ae_statistics(dataset_name, ae_name, group=group, epoch=epoch, lams=lams, eps=eps, metrics=metrics, device='cuda:{}'.format(device))\n",
    "\n",
    "        path = os.path.join('../saves/image/', dataset_name, group)\n",
    "        for i, nm in enumerate(names):\n",
    "            ls = []\n",
    "            for each in stats:\n",
    "                ls.append(each[i])\n",
    "            np.save(os.path.join(path, 'e{}_{}.npy'.format(eps, nm)), torch.stack(ls).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../saves/image/syn/vol/vol_0.01_e3.0/\n",
      "../saves/image/syn/vol/vol_0.01_e0.0/\n",
      "../saves/image/syn/vol/vol_0.01_e10.0/\n",
      "../saves/image/syn/vol/vol_0.01_e30.0/\n",
      "../saves/image/syn/vol/vol_0.01_e1.0/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 40.52it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 35.30it/s] \n",
      "100%|██████████| 50/50 [00:01<00:00, 28.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../saves/image/syn/vol/vol_0.003_e3.0/\n"
     ]
    }
   ],
   "source": [
    "for i, eps in zip(range(5), [0., 1., 3., 10., 30.]):\n",
    "    p = mp.Process(target=vol_main, args=(i, eps))\n",
    "    p.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toilet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
